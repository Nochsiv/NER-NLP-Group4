{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2TokenizerFast, AdamW, get_linear_schedule_with_warmup, AutoModelForTokenClassification\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (28931, 3)\n",
      "Dev size: (3460, 3)\n",
      "Test size: (4143, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "dev_data = pd.read_csv(\"dev.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Dataset sizes\n",
    "print(\"Train size:\", train_data.shape)\n",
    "print(\"Dev size:\", dev_data.shape)\n",
    "print(\"Test size:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Combine Data for Global Analysis and Visualize Tag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Distribution Across Splits:\n",
      " tag       B     I      O\n",
      "split                   \n",
      "dev     409   405   2646\n",
      "test    451   488   3204\n",
      "train  3440  3562  21929\n"
     ]
    }
   ],
   "source": [
    "# Combine for global analysis\n",
    "all_data = pd.concat([train_data.assign(split=\"train\"),\n",
    "                      dev_data.assign(split=\"dev\"),\n",
    "                      test_data.assign(split=\"test\")])\n",
    "\n",
    "# Tag distribution per split\n",
    "tag_distribution = all_data.groupby('split')['tag'].value_counts().unstack().fillna(0)\n",
    "print(\"Tag Distribution Across Splits:\\n\", tag_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Load Tokenizer (GPT-2 with add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer with add_prefix_space=True for pre-tokenized inputs\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Preprocess Tags (Convert 'b' to 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tags to indices\n",
    "tag2id = {\"O\": 0, \"B\": 1, \"I\": 2}\n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "\n",
    "# Function to preprocess tags and convert 'b' to 'B'\n",
    "def preprocess_tags(df):\n",
    "    df['tag'] = df['tag'].apply(lambda x: 'B' if x == 'b' else x)\n",
    "    return df\n",
    "\n",
    "# Preprocess the tags before tokenization\n",
    "train_data = preprocess_tags(train_data)\n",
    "dev_data = preprocess_tags(dev_data)\n",
    "test_data = preprocess_tags(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Tokenize and Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(df):\n",
    "    tokenized_data = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    # Group by abstract_id to process abstracts as units\n",
    "    grouped = df.groupby(\"abstract_id\")\n",
    "\n",
    "    for abstract_id, group in grouped:\n",
    "        words = group[\"word\"].tolist()\n",
    "        tags = group[\"tag\"].tolist()\n",
    "\n",
    "        # Tokenize words with pre-tokenized inputs (add prefix space)\n",
    "        tokenized = tokenizer(words, truncation=True, is_split_into_words=True)\n",
    "\n",
    "        # Align labels\n",
    "        labels = []\n",
    "        word_ids = tokenized.word_ids()\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  # Special tokens\n",
    "            else:\n",
    "                tag = tags[word_idx]\n",
    "                labels.append(tag2id.get(tag, -100))  # Default to -100 for unexpected tags\n",
    "\n",
    "        tokenized_data[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        tokenized_data[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        tokenized_data[\"labels\"].append(labels)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "# Process datasets\n",
    "try:\n",
    "    train_tokenized = tokenize_and_align_labels(train_data)\n",
    "    dev_tokenized = tokenize_and_align_labels(dev_data)\n",
    "    test_tokenized = tokenize_and_align_labels(test_data)\n",
    "    print(\"Tokenization completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during tokenization:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Create a Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class to handle tokenized data\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_masks = tokenized_data['attention_mask']\n",
    "        self.labels = tokenized_data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Convert the tokenized data into Dataset objects\n",
    "train_dataset = NERDataset(train_tokenized)\n",
    "dev_dataset = NERDataset(dev_tokenized)\n",
    "test_dataset = NERDataset(test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForTokenClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model (DistilGPT-2)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Set the model to train mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Create Custom Collate Function to Pad Sequences\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom collate function to pad sequences\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Ensure tokenizer's pad_token_id is not None\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Pad labels with -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create DataLoader for training and validation sets using the custom collate function\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Set Up Loss Function, Optimizer, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Class weights (adjust based on your dataset)\n",
    "class_weights = torch.tensor([1.0, 10.0, 10.0])  # 'O' : 1, 'B' : 10, 'I' : 10\n",
    "\n",
    "# Loss function with class weights\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_dataloader) * 50  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 18/18 [01:35<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 18/18 [01:31<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 18/18 [01:32<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 18/18 [01:34<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.4025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 18/18 [01:35<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.3606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 18/18 [01:37<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Average Training Loss: 0.3419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Average Training Loss: 0.3194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Average Training Loss: 0.2888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/50: 100%|██████████| 18/18 [01:33<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Average Training Loss: 0.2733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/50: 100%|██████████| 18/18 [01:31<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Average Training Loss: 0.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/50: 100%|██████████| 18/18 [01:29<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Average Training Loss: 0.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Average Training Loss: 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Average Training Loss: 0.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/50: 100%|██████████| 18/18 [01:36<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Average Training Loss: 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Average Training Loss: 0.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/50: 100%|██████████| 18/18 [01:30<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Average Training Loss: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/50: 100%|██████████| 18/18 [01:29<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Average Training Loss: 0.1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/50: 100%|██████████| 18/18 [01:30<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Average Training Loss: 0.1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/50: 100%|██████████| 18/18 [01:31<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Average Training Loss: 0.1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/50: 100%|██████████| 18/18 [01:35<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Average Training Loss: 0.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/50: 100%|██████████| 18/18 [01:33<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Average Training Loss: 0.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/50: 100%|██████████| 18/18 [01:30<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Average Training Loss: 0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/50: 100%|██████████| 18/18 [01:29<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Average Training Loss: 0.1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/50: 100%|██████████| 18/18 [01:32<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Average Training Loss: 0.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/50: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Average Training Loss: 0.1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/50: 100%|██████████| 18/18 [01:31<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Average Training Loss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/50: 100%|██████████| 18/18 [01:34<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Average Training Loss: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/50: 100%|██████████| 18/18 [01:35<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Average Training Loss: 0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/50: 100%|██████████| 18/18 [01:32<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Average Training Loss: 0.0953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/50: 100%|██████████| 18/18 [01:30<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Average Training Loss: 0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Average Training Loss: 0.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32/50: 100%|██████████| 18/18 [01:30<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Average Training Loss: 0.0908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33/50: 100%|██████████| 18/18 [01:32<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Average Training Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34/50: 100%|██████████| 18/18 [01:35<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Average Training Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35/50: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Average Training Loss: 0.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36/50: 100%|██████████| 18/18 [01:32<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Average Training Loss: 0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37/50: 100%|██████████| 18/18 [01:31<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Average Training Loss: 0.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38/50: 100%|██████████| 18/18 [01:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Average Training Loss: 0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39/50: 100%|██████████| 18/18 [01:31<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Average Training Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40/50: 100%|██████████| 18/18 [01:29<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Average Training Loss: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41/50: 100%|██████████| 18/18 [01:32<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Average Training Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42/50: 100%|██████████| 18/18 [01:28<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Average Training Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43/50: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Average Training Loss: 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44/50: 100%|██████████| 18/18 [01:28<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Average Training Loss: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45/50: 100%|██████████| 18/18 [01:29<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Average Training Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46/50: 100%|██████████| 18/18 [01:28<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Average Training Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47/50: 100%|██████████| 18/18 [01:28<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Average Training Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48/50: 100%|██████████| 18/18 [01:32<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Average Training Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49/50: 100%|██████████| 18/18 [01:34<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Average Training Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50/50: 100%|██████████| 18/18 [01:31<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Average Training Loss: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.85      0.90      2747\n",
      "           B       0.61      0.81      0.69       604\n",
      "           I       0.65      0.78      0.71       496\n",
      "\n",
      "    accuracy                           0.83      3847\n",
      "   macro avg       0.74      0.81      0.77      3847\n",
      "weighted avg       0.86      0.83      0.84      3847\n",
      "\n",
      "Accuracy: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Collect predictions\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Flatten the lists\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Mask out padding tokens\n",
    "    mask = all_labels != -100\n",
    "    all_preds = all_preds[mask]\n",
    "    all_labels = all_labels[mask]\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=list(id2tag.values())))\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = np.sum(all_preds == all_labels) / len(all_labels)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the dev set\n",
    "evaluate(model, dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tep 13: Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/path/to/save/tokenizer\\\\tokenizer_config.json',\n",
       " '/path/to/save/tokenizer\\\\special_tokens_map.json',\n",
       " '/path/to/save/tokenizer\\\\vocab.json',\n",
       " '/path/to/save/tokenizer\\\\merges.txt',\n",
       " '/path/to/save/tokenizer\\\\added_tokens.json',\n",
       " '/path/to/save/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"/path/to/save/model\")\n",
    "tokenizer.save_pretrained(\"/path/to/save/tokenizer\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING WITH REAL ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP: B\n",
      "bridges: I\n",
      "the: O\n",
      "gap: O\n",
      "between: O\n",
      "human: B\n",
      "communication: I\n",
      "and: O\n",
      "computer: B\n",
      "understanding: I\n",
      "by: O\n",
      "analyzing: O\n",
      "and: O\n",
      "interpreting: O\n",
      "language: O\n",
      "data.: O\n",
      "Sentiment: B\n",
      "analysis,: I\n",
      "a: O\n",
      "popular: O\n",
      "application: O\n",
      "of: O\n",
      "NLP,: B\n",
      "helps: O\n",
      "companies: O\n",
      "understand: O\n",
      "customer: B\n",
      "opinions: O\n",
      "from: O\n",
      "reviews: O\n",
      "and: O\n",
      "social: B\n",
      "media: O\n",
      "posts.: O\n"
     ]
    }
   ],
   "source": [
    "# Function to predict entities and align tokens with their tags\n",
    "def predict_entities(sentence):\n",
    "    # Ensure the tokenizer has a pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding\n",
    "    \n",
    "    # Tokenize the sentence and get the predicted entity tags\n",
    "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, is_split_into_words=False)\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "\n",
    "    # Align tokens with predicted labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    predicted_tags = [id2tag[pred] for pred in predictions]\n",
    "\n",
    "    # Merge subwords with the same entity tag\n",
    "    result = []\n",
    "    previous_tag = None\n",
    "    current_word = []\n",
    "    \n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        clean_token = token.lstrip(\"Ġ\")  # Remove leading 'Ġ' character\n",
    "        \n",
    "        # If it's a subword token (part of a larger word), we append it to the current word\n",
    "        if token.startswith(\"Ġ\"):\n",
    "            if current_word:\n",
    "                result.append((\"\".join(current_word), previous_tag))\n",
    "            current_word = [clean_token]\n",
    "            previous_tag = tag\n",
    "        else:\n",
    "            current_word.append(clean_token)\n",
    "\n",
    "    if current_word:\n",
    "        result.append((\"\".join(current_word), previous_tag))  # Add the last word\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example sentence to test the model\n",
    "sentence = \"NLP bridges the gap between human communication and computer understanding by analyzing and interpreting language data. Sentiment analysis, a popular application of NLP, helps companies understand customer opinions from reviews and social media posts.\"\n",
    "\n",
    "# Get predicted entities for the sentence\n",
    "predicted_entities = predict_entities(sentence)\n",
    "\n",
    "# Print the results without 'Ġ'\n",
    "for token, tag in predicted_entities:\n",
    "    print(f\"{token}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved as 'distilgpt2.pkl'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Define id2tag and tag2id (adjust these based on your dataset)\n",
    "id2tag = {0: \"O\", 1: \"B\", 2: \"I\"}  # Example mapping\n",
    "tag2id = {v: k for k, v in id2tag.items()}\n",
    "\n",
    "# Load the tokenizer and model architecture\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilgpt2\", num_labels=len(id2tag))\n",
    "\n",
    "# Then proceed to save the model and tokenizer\n",
    "model_data = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"id2tag\": id2tag,\n",
    "    \"tag2id\": tag2id\n",
    "}\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"distilgpt2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model and tokenizer saved as 'distilgpt2.pkl'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
